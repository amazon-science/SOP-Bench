# SOP-Bench : Complex Industrial SOPs for Evaluating LLM Agents

<img width="613" height="406" alt="image" src="https://github.com/user-attachments/assets/db61aa12-b64e-47e6-b71d-01aa80c3124e" />

## Overview
**SOP-Bench** is a comprehensive benchmark for evaluating LLM-based agents on complex, multi-step Standard Operating Procedures (SOPs) that are fundamental to industrial automation. Built from 2,000+ tasks across 12 industrial domains (healthcare, logistics, finance, content moderation, etc.), SOP-Bench addresses the gap between existing benchmarks and real-world procedural complexity.

### Key Features
ğŸ­ **Human Expert-Authored SOPs**: Authentic procedures crafted by domain experts reflecting real-world complexity
ğŸ¤– **Human-AI Collaborative Framework**: AI-generated artifacts (tools, APIs, datasets) with human validation
ğŸ“Š **Executable Interfaces**: Ground-truth outputs enabling reproducible evaluation
ğŸ”§ **Two Agent Architectures**: Function-Calling (FC) and ReAct agents for systematic comparison
ğŸ“ˆ **11 Frontier Models Evaluated**: Comprehensive benchmarking across Claude, GPT, Llama, and DeepSeek families

## News

- **[2026-02]** ğŸ‰ SOP-Bench submitted to KDD 2026 Datasets and Benchmarks Track.


Be sure to:

* Change the title in this README
* Edit your repository description on GitHub
* Write in your license below and create a LICENSE file

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This library is licensed under the LICENSE NAME HERE License.

